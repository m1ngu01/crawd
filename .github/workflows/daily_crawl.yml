name: Daily Crawl

on:
  schedule:
    - cron: "0 0 * * *"  # UTC 기준 매일 00:00
  workflow_dispatch: {}

jobs:
  crawl:
    runs-on: ubuntu-latest
    timeout-minutes: 120
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Setup Chrome
        uses: browser-actions/setup-chrome@v1

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r workflowP/requirements.txt

      - name: Heartbeat start
        shell: bash
        run: |
          set -euo pipefail
          ( while sleep 300; do echo "::notice::heartbeat $(date -Is)"; done ) &
          echo $! > .hb.pid

      - name: Run daily crawl
        shell: bash
        env:
          # 설정값 환경변수 (필요 시 조정)
          WORKERS: '2'
          SAMPLE_N: '1250'
          PAGELOAD_TIMEOUT: '15'
          IMPLICIT_WAIT: '2'
          WAIT_TIMEOUT: '10'
          SCRIPT_TIMEOUT: '6000'
          PYTHONUNBUFFERED: '1'
        run: |
          set -euo pipefail
          timeout 110m python workflowP/daily_crawl.py

      - name: Heartbeat stop
        if: always()
        shell: bash
        run: |
          if [ -f .hb.pid ]; then kill $(cat .hb.pid) || true; fi

      - name: Upload crawl logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: daily-crawl-logs
          path: |
            workflowP/daily_crawl.log
            workflowP/craw/data/quick_text_probe_parallel.json
          if-no-files-found: warn
