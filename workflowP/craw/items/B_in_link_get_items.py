import logging
import json
import re
import time
import os
import datetime
from selenium.webdriver.chrome.service import Service
from pathlib import Path
from multiprocessing import Pool, cpu_count, Manager
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from A_link_filter import to_list

# ================== ìƒìˆ˜ ==================
THIS_FILE = Path(__file__).resolve()
PROJ_ROOT = THIS_FILE.parents[2]
DATA_DIR = PROJ_ROOT / "craw" / "data"
DATA_DIR.mkdir(parents=True, exist_ok=True)
OUTPUT_DIR = DATA_DIR / "quick_text_probe_parallel"
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
LEGACY_JSON_PATH = DATA_DIR / "quick_text_probe_parallel.json"
MANIFEST_PATH = OUTPUT_DIR / "manifest.json"
STATE_PATH = OUTPUT_DIR / "state.json"
STATUS_PATH = DATA_DIR / "quick_text_probe_parallel.status.json"
JSON_PART_RECORDS = max(1, int(os.environ.get("JSON_PART_RECORDS", "500")))

PAGELOAD_TIMEOUT = int(os.environ.get("PAGELOAD_TIMEOUT", "10"))
IMPLICIT_WAIT = int(os.environ.get("IMPLICIT_WAIT", "2"))
WAIT_TIMEOUT = int(os.environ.get("WAIT_TIMEOUT", "10"))
SAMPLE_N = int(os.environ.get("SAMPLE_N", "7000"))
WORKERS = int(os.environ.get("WORKERS", "4"))  # ë³‘ë ¬ ì‹¤í–‰ ê°œìˆ˜
CHECKPOINT_N = int(os.environ.get("CHECKPOINT_N", "10"))  # ì¤‘ê°„ ì €ì¥ ë‹¨ìœ„
# ë°°ì¹˜(ì²­í¬) í¬ê¸°: ê¸°ë³¸ 10 â†’ 10ê°œ ë‹¨ìœ„ë¡œ ë¶€ëª¨ê°€ ê²°ê³¼ ìˆ˜ì‹ /ì²´í¬í¬ì¸íŠ¸ ê°€ëŠ¥
BATCH_SIZE = int(os.environ.get("BATCH_SIZE", "10"))

LIST_SELECTORS = [
    "div.main_prodlist.main_prodlist_list > ul > li"
]
# ëª©ë¡í˜• ë³´ê¸° ë²„íŠ¼ (ë¦¬ìŠ¤íŠ¸í˜• ì „í™˜)
LIST_VIEW_BUTTON_SELECTOR = (
    "#danawa_content > div.product_list_wrap > div > div.prod_list_tab > div > "
    "div.view_opt > ul > li.type_item"
)

# ================== ë¡œê¹… ==================
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s][%(processName)s] %(message)s",
    datefmt="%H:%M:%S",
)
log = logging.getLogger(__name__)

# ================== ìœ í‹¸ ==================
def clean_text(s: str) -> str:
    """í…ìŠ¤íŠ¸ ì •ì œ"""
    if not s:
        return ""
    s = re.sub(r"[ \t\r\f]+", " ", s)
    s = re.sub(r"\n{3,}", "\n\n", s)
    return s.strip()

def parse_float(value: str):
    """ë¬¸ìì—´ì—ì„œ ì‹¤ìˆ˜ ì¶”ì¶œ (ì—†ìœ¼ë©´ None)"""
    if not value:
        return None
    cleaned = value.replace(",", "")
    match = re.search(r"\d+(?:\.\d+)?", cleaned)
    if not match:
        return None
    try:
        return float(match.group())
    except ValueError:
        return None

def parse_int(value: str):
    """ë¬¸ìì—´ì—ì„œ ì •ìˆ˜ ì¶”ì¶œ (ì—†ìœ¼ë©´ None)"""
    if not value:
        return None
    digits = re.sub(r"[^\d]", "", value)
    if not digits:
        return None
    try:
        return int(digits)
    except ValueError:
        return None

def short_exception(exc: Exception) -> str:
    """
    Selenium ì˜ˆì™¸ ë“±ì—ì„œ ìŠ¤íƒíŠ¸ë ˆì´ìŠ¤ê°€ í¬í•¨ëœ ë©”ì‹œì§€ë¥¼ ê°„ê²°í•˜ê²Œ ì •ë¦¬í•œë‹¤.
    """
    text = ""
    for attr in ("msg", "message"):
        candidate = getattr(exc, attr, None)
        if isinstance(candidate, str) and candidate.strip():
            text = candidate
            break
    if not text:
        text = str(exc) if exc else ""
    if "Stacktrace:" in text:
        text = text.split("Stacktrace:", 1)[0]
    if text.lower().startswith("message"):
        parts = text.split(":", 1)
        if len(parts) == 2:
            text = parts[1]
    text = text.strip()
    if not text:
        text = exc.__class__.__name__ if exc else ""
    return text

def ensure_list_view(driver, page_url=None):
    """ëª©ë¡í˜•(ë¦¬ìŠ¤íŠ¸) ë³´ê¸°ë¡œ ì „í™˜"""
    if not page_url:
        try:
            page_url = driver.current_url
        except Exception:
            page_url = None
    url_for_log = page_url or "<unknown>"

    try:
        list_button = WebDriverWait(driver, WAIT_TIMEOUT).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, LIST_VIEW_BUTTON_SELECTOR))
        )
    except Exception as exc:
        log.warning("ëª©ë¡í˜• ë³´ê¸° íƒ­ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤ (url=%s): %s", url_for_log, short_exception(exc))
        return False

    try:
        current_class = list_button.get_attribute("class") or ""
        if "selected" in current_class.split():
            return True

        driver.execute_script("arguments[0].click();", list_button)

        def _list_view_selected(d):
            try:
                refreshed = d.find_element(By.CSS_SELECTOR, LIST_VIEW_BUTTON_SELECTOR)
                cls = refreshed.get_attribute("class") or ""
                return "selected" in cls.split()
            except Exception:
                return False

        WebDriverWait(driver, WAIT_TIMEOUT).until(_list_view_selected)
        time.sleep(0.5)
        return True
    except Exception as exc:
        log.warning("ëª©ë¡í˜• ë³´ê¸° ì „í™˜ ì‹¤íŒ¨ (url=%s): %s", url_for_log, short_exception(exc))
        return False

def find_product_items(driver):
    """ìƒí’ˆ ë¦¬ìŠ¤íŠ¸ íƒìƒ‰ (ë¡œë“œ ëŒ€ê¸° í¬í•¨)"""
    for sel in LIST_SELECTORS:
        try:
            WebDriverWait(driver, WAIT_TIMEOUT).until(
                EC.presence_of_all_elements_located((By.CSS_SELECTOR, sel))
            )
            els = driver.find_elements(By.CSS_SELECTOR, sel)
            if els:
                return els, sel
        except Exception:
            pass
    return [], ""

# ================== ì›Œì»¤ í•¨ìˆ˜ ==================
def worker(args):
    """ë§í¬ ë¦¬ìŠ¤íŠ¸ í•œ ë¬¶ìŒì„ ë³‘ë ¬ë¡œ í¬ë¡¤ë§"""
    if len(args) == 4:
        link_batch, start_index, total, skipped = args
    else:
        link_batch, start_index, total = args
        skipped = 0

    progress_total = total - skipped
    if progress_total <= 0:
        progress_total = len(link_batch) or 1

    if skipped:
        progress_total_display = f"{total - skipped}"
    else:
        progress_total_display = str(progress_total)
    results = []

    # í¬ë¡¬ ì˜µì…˜ ì„¤ì •
    service = Service(log_path=os.devnull)
    options = Options()
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--disable-gpu")
    options.add_argument("--disable-software-rasterizer")
    options.add_argument("--window-size=1400,1000")
    options.add_argument("--headless=new")
    options.add_argument("--log-level=3")
    options.add_argument("--silent")
    options.add_argument("--disable-logging")
    options.add_argument("--disable-extensions")
    options.add_argument("--disable-notifications")
    options.add_argument("--disable-default-apps")
    options.add_argument("--disable-breakpad")
    options.add_argument("--no-default-browser-check")
    options.add_argument("--no-first-run")
    options.add_argument("--mute-audio")

    options.add_experimental_option("excludeSwitches", [
        "enable-logging",
        "enable-automation",
        "enable-blink-features=AutomationControlled"
    ])
    options.add_experimental_option("useAutomationExtension", False)

    driver = webdriver.Chrome(service=service, options=options)
    driver.set_page_load_timeout(PAGELOAD_TIMEOUT)
    driver.implicitly_wait(IMPLICIT_WAIT)

    # ì§„í–‰ë„ ì¶œë ¥ í­ ê³„ì‚° (ì˜ˆ: 1250 -> í­ 5 ì— ë§ì¶° ìš°ì¸¡ ì–¸ë”ìŠ¤ì½”ì–´ íŒ¨ë”©)
    width = max(5, len(str(progress_total)))

    for idx, r in enumerate(link_batch, start=0):
        cur = start_index + idx
        cur_disp = f"{cur}{' ' * (width - len(str(cur)))}"
        prog_str = f"ì§„í–‰ë„ [{cur_disp}/ {progress_total_display}]"
        link = r.get("link")
        path = [r.get(f"{i}ì°¨", "") for i in range(1, 5)]
        result = {"link": link, "path": path, "ok": False, "products": []}

        try:
            driver.get(link)
            time.sleep(2)

            ensure_list_view(driver, page_url=link)

            # ìƒí’ˆ ë¦¬ìŠ¤íŠ¸ íƒìƒ‰
            items, used_sel = find_product_items(driver)
            if not items:
                continue

            for item in items[:30]:  # ìµœëŒ€ 30ê°œ
                try:
                    # ================== ì´ë¯¸ì§€ ==================
                    img_el = item.find_element(
                        By.CSS_SELECTOR,
                        "div.prod_main_info > div.thumb_image > a.thumb_link > img"
                    )
                    image = img_el.get_attribute("data-original") or img_el.get_attribute("src")

                    # ================== ìƒí’ˆëª… ==================
                    prod_anchor = item.find_element(
                        By.CSS_SELECTOR,
                        "div.prod_main_info > div.prod_info > p > a"
                    )
                    prod_name = clean_text(prod_anchor.text)
                    prod_link = prod_anchor.get_attribute("href") or ""

                    # ================== ìŠ¤í™/íƒœê·¸ ==================
                    tags_css = ", ".join([
                        "div.prod_main_info div.prod_info div.spec-box[data-simple-description-open-area='Y'] div.spec_list",
                        "div.prod_main_info div.prod_info div.spec-box:not([style*='display:none']) div.spec_list",
                        "div.prod_info div.spec-box[data-simple-description-open-area='Y'] div.spec_list",
                        "div.prod_info div.spec-box:not([style*='display:none']) div.spec_list",
                    ])
                    tags_elem = item.find_elements(By.CSS_SELECTOR, tags_css)
                    tags = clean_text(tags_elem[0].text if tags_elem else "")

                    # ================== ê°€ê²© ==================
                    price_els = item.find_elements(
                        By.CSS_SELECTOR,
                        "div.prod_main_info > div.prod_pricelist > ul > li p.price_sect > a > strong"
                    )
                    price = clean_text(price_els[0].text if price_els else "")

                    # ================== í‰ì /ë¦¬ë·° ==================
                    score_els = item.find_elements(
                        By.CSS_SELECTOR,
                        "div.prod_info > div.prod_sub_info > div > div > a > div > span.text__score"
                    )
                    raw_score = clean_text(score_els[0].text if score_els else "")
                    rating = parse_float(raw_score)

                    review_els = item.find_elements(
                        By.CSS_SELECTOR,
                        "div.prod_info > div.prod_sub_info > div > div > a > div > div.text__review > span.text__number"
                    )
                    raw_review_count = clean_text(review_els[0].text if review_els else "")
                    review_count = parse_int(raw_review_count)

                    rating_weighted = None
                    if rating is not None and review_count is not None:
                        rating_weighted = round(rating * review_count, 2)

                    # ================== ì €ì¥ ==================
                    result["products"].append({
                        "link": prod_link,
                        "image": image,
                        "prod_name": prod_name,
                        "tags": tags,
                        "price": price,
                        "rating": rating,
                        "review_count": review_count,
                        "rating_weighted": rating_weighted,
                        "raw_rating_text": raw_score,
                        "raw_review_text": raw_review_count,
                    })

                except Exception as e:
                    log.debug("item parse error: %s", e, exc_info=True)
                    continue

            result.update({
                "ok": True,
                "list_selector": used_sel,
                "product_count": len(result["products"])
            })
            results.append(result)
            log.info(f"âœ… {len(result['products'])}ê°œ ì™„ë£Œ | {prog_str} - {path[1] if len(path) > 1 else path[0]}")

        except Exception as e:
            log.warning(f"âŒ {path[-1] if path[-1] else link} ì—ëŸ¬: {short_exception(e)}")

    driver.quit()
    return results

# ================== ë©”ì¸ ==================
def _read_existing_results():
    """
    ê¸°ì¡´ ê²°ê³¼ë¥¼ ë¡œë“œí•œë‹¤. ë¶„í•  ì €ì¥(manifest ê¸°ë°˜) ë˜ëŠ” ë ˆê±°ì‹œ ë‹¨ì¼ JSON ëª¨ë‘ ì§€ì›.
    """
    results = []
    manifest = None
    if MANIFEST_PATH.exists():
        try:
            with MANIFEST_PATH.open("r", encoding="utf-8") as mf:
                manifest = json.load(mf)
        except Exception as exc:
            log.warning("manifest ì½ê¸° ì‹¤íŒ¨: %s", exc)
            manifest = None
        if manifest:
            for part in manifest.get("parts", []):
                filename = part.get("file")
                if not filename:
                    continue
                part_path = OUTPUT_DIR / filename
                if not part_path.exists():
                    continue
                with part_path.open("r", encoding="utf-8") as pf:
                    for line in pf:
                        line = line.strip()
                        if not line:
                            continue
                        try:
                            results.append(json.loads(line))
                        except json.JSONDecodeError:
                            continue
            return results
    if LEGACY_JSON_PATH.exists():
        try:
            with LEGACY_JSON_PATH.open("r", encoding="utf-8") as f:
                return json.load(f)
        except Exception as exc:
            log.warning("ë ˆê±°ì‹œ JSON ë¡œë“œ ì‹¤íŒ¨: %s", exc)
            return []
    return results

def _chunk_list(items, size):
    for idx in range(0, len(items), size):
        yield items[idx: idx + size]

def _write_sharded_results(data):
    """
    ë°ì´í„°ë¥¼ JSONL íŒŒíŠ¸ë¡œ ë¶„í•  ì €ì¥í•˜ê³  manifest/state/statusë¥¼ ê°±ì‹ í•œë‹¤.
    """
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    timestamp = datetime.datetime.now().isoformat()
    tmp_parts = []
    part_entries = []
    for index, chunk in enumerate(_chunk_list(data, JSON_PART_RECORDS), start=1):
        filename = f"part_{index:05}.jsonl"
        final_path = OUTPUT_DIR / filename
        tmp_path = final_path.with_suffix(final_path.suffix + ".tmp")
        with tmp_path.open("w", encoding="utf-8") as f:
            for row in chunk:
                f.write(json.dumps(row, ensure_ascii=False))
                f.write("\n")
        tmp_parts.append((tmp_path, final_path))
        part_entries.append({"file": filename, "count": len(chunk)})

    for existing in OUTPUT_DIR.glob("part_*.jsonl"):
        try:
            existing.unlink()
        except OSError:
            pass

    for tmp_path, final_path in tmp_parts:
        tmp_path.replace(final_path)

    manifest = {
        "parts": part_entries,
        "total_count": len(data),
        "updated_at": timestamp,
        "part_size_limit": JSON_PART_RECORDS,
    }
    tmp_manifest = MANIFEST_PATH.with_suffix(".tmp")
    with tmp_manifest.open("w", encoding="utf-8") as mf:
        json.dump(manifest, mf, indent=2, ensure_ascii=False)
    tmp_manifest.replace(MANIFEST_PATH)

    state = {
        "links": [item.get("link") for item in data if isinstance(item, dict) and item.get("ok")],
        "updated_at": timestamp,
    }
    tmp_state = STATE_PATH.with_suffix(".tmp")
    with tmp_state.open("w", encoding="utf-8") as sf:
        json.dump(state, sf, indent=2, ensure_ascii=False)
    tmp_state.replace(STATE_PATH)

    if LEGACY_JSON_PATH.exists():
        try:
            LEGACY_JSON_PATH.unlink()
        except OSError:
            pass

def _write_status(processed_links, pending_links, skipped_links, total_links, eligible_links, complete_total):
    payload = {
        "timestamp": datetime.datetime.now().isoformat(),
        "processed_links": processed_links,
        "pending_links": pending_links,
        "skipped_links": skipped_links,
        "total_links": total_links,
        "eligible_links": eligible_links,
        "complete_total": complete_total,
    }
    tmp_status = STATUS_PATH.with_suffix(".tmp")
    with tmp_status.open("w", encoding="utf-8") as f:
        json.dump(payload, f, indent=2, ensure_ascii=False)
    tmp_status.replace(STATUS_PATH)

def main():
    rows = to_list()
    if not rows:
        log.warning("í•„í„°ëœ ë§í¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    # ì¤‘ë³µ ì œê±°
    uniq, seen = [], set()
    for r in rows:
        lk = r.get("link", "")
        if lk and lk not in seen:
            uniq.append(r)
            seen.add(lk)

    # ğŸ”¹ ê¸°ì¡´ ê²°ê³¼ ë¡œë“œ ë° ì¬ì‹œì‘ ìŠ¤í‚µ êµ¬ì„±
    prev_results = _read_existing_results()
    prev_links = {r.get("link") for r in prev_results if isinstance(r, dict) and r.get("ok")}

    # ğŸ”¹ ì²˜ë¦¬ ê°œìˆ˜ ì œí•œ (deterministic)
    total = min(SAMPLE_N, len(uniq)) if SAMPLE_N > 0 else len(uniq)
    uniq = uniq[:total]

    # ğŸ”¹ ì¬ì‹œì‘ ìŠ¤í‚µ ì ìš©
    todo = [r for r in uniq if r.get("link") not in prev_links]
    skipped = len(uniq) - len(todo)
    log.info(f"ì´ {len(rows)}ê°œ ì¤‘ ìƒìœ„ {total}ê°œ ë§í¬ ë³‘ë ¬ ì ê²€ ì‹œì‘ (ì´ì „ ì™„ë£Œ {skipped}ê°œ ìŠ¤í‚µ)")

    # ğŸ”¹ ë³‘ë ¬ ì²˜ë¦¬ ë¶„í• 
    chunk_size = max(1, min(BATCH_SIZE, len(todo))) if todo else 1
    raw_chunks = [todo[i:i + chunk_size] for i in range(0, len(todo), chunk_size)]
    # ê° ì²­í¬ì˜ ì‹œì‘ ì¸ë±ìŠ¤(1-based)ì™€ ì´ ê°œìˆ˜ë¥¼ í•¨ê»˜ ì „ë‹¬í•˜ì—¬ ì „ì—­ ì§„í–‰ë„ë¥¼ ê³„ì‚°
    chunks = []
    start = 1
    for batch in raw_chunks:
        chunks.append((batch, start, total, skipped))
        start += len(batch)
    log.info(f"ê° í”„ë¡œì„¸ìŠ¤ë‹¹ {chunk_size}ê°œ ë§í¬ ì²˜ë¦¬ ì˜ˆì •")

    # ğŸ”¹ ë³‘ë ¬ ì‹¤í–‰
    manager = Manager()
    shared_results = manager.list()  # ë³‘ë ¬ ì•ˆì „ ìˆ˜ì§‘
    lock = manager.Lock()

    pending_initial = len(todo)
    last_checkpoint_at = 0

    def _maybe_checkpoint():
        nonlocal last_checkpoint_at
        current_total = len(prev_results) + len(shared_results)
        if CHECKPOINT_N > 0 and current_total - last_checkpoint_at >= CHECKPOINT_N:
            with lock:
                current_shared = list(shared_results)
                data = list(prev_results) + current_shared
                _write_sharded_results(data)
                last_checkpoint_at = current_total
                pending_links = max(0, pending_initial - len(current_shared))
                _write_status(len(current_shared), pending_links, skipped, len(rows), len(uniq), len(data))
                log.info(f"ğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ({current_total}ê°œ) â†’ {OUTPUT_DIR}")

    if todo:
        with Pool(WORKERS) as pool:
            for batch_results in pool.imap_unordered(worker, chunks):
                with lock:
                    for item in batch_results:
                        shared_results.append(item)
                _maybe_checkpoint()

    # ğŸ”¹ ìµœì¢… ì €ì¥ (ì´ì „ + ì‹ ê·œ)
    final_shared = list(shared_results)
    final_data = list(prev_results) + final_shared
    force_write = bool(final_shared) or not MANIFEST_PATH.exists() or LEGACY_JSON_PATH.exists()
    if force_write:
        _write_sharded_results(final_data)
    else:
        log.info("ğŸ’¾ ì‹ ê·œ ê²°ê³¼ ì—†ìŒ, ê¸°ì¡´ ë¶„í•  íŒŒì¼ ìœ ì§€")
    pending_links = max(0, pending_initial - len(final_shared))
    _write_status(len(final_shared), pending_links, skipped, len(rows), len(uniq), len(final_data))
    log.info(f"âœ… ë³‘ë ¬ í¬ë¡¤ë§ ì™„ë£Œ: ì‹ ê·œ {len(final_shared)}ê°œ, ëˆ„ì  {len(final_data)}ê°œ ì €ì¥ â†’ {OUTPUT_DIR}")

"""ë‹¨ì¼ ì‹¤í–‰ ì—”íŠ¸ë¦¬"""
if __name__ == "__main__":
    main()
